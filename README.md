## Instance augmentation code and experiments

### The seq2seq code used in this project is adapted from the pytorch documentation:
http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

the original version can be found in the tutorials subdirectory.

### Using wikipedia:
1) Download wiki dump (pages-articles) and bunzip
2) Clone the wikiextractor repo (https://github.com/attardi/wikiextractor.git)
3) From that directory, run:
```python WikiExtractor.py -o <output directory> -q --filter_disambig_pages --no-templates <wiki xml location>```

This will create sub-directories in the output directory specified called AA, AB, etc. They look like xml but they are not:
https://github.com/attardi/wikiextractor/issues/6

### Writing to one document-per-line tokenized/sentencized/paragraphized format:
```python extract_wiki_to_documents.py <one of the files generated by wikiextractor>```

### To do it for a whole directory:
```for i in `find <wiki extractor output dir>/AA/ -name wiki*`; do python extract_wiki_to_documents.py $i > <output directory>/AA_`basename $i`.txt; done```

### To get a file of one sentence per line 
```python doclines2sentlines.py <doclines directory> > simple.sentlines.txt```

### shuffle and remove duplicate lines:
```sort -u simple.sentlines.txt | sort -R > simple.sentlines.shuffled.txt```

### I ran with only 100k sentences to start:
```head -100000 simple.sentlines.shuffled.txt > simple.sentlines.shuffled.100k.txt```
